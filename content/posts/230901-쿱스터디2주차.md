---
title: "230901 쿱스터디2주차"
date: 2023-08-28T20:55:38+09:00
draft: false
---


## 12. Docker-vs-ContainerD

- 쿠버네티스
 - rkt(로켓)
 - 컨테이너디
 - 도커

### 컨테이너디 (이하 컨디)
- 컨테이너디 CIL : ctr
- 도커링 비슷한 crt이 nerdctl / 범생이ctr
- 예시
```
docker
= nerdctl

docker run --name redis redis:alpine
= nerdctl run --name redis redis:alpine

docker run --name webserver -p 80:80 -d nginx
= nerdctl run --name webserver -p 80:80 -d nginx
```

### crictl
- CLI for CRI 호환 컨테이너 런타임
- 검사하고 디버깅에 사용
- 컨디 커뮤니티를 위해 작성됨
- 디버깅툴임! 큐블렛이랑 잘어울림
- 예시
```
crictl pull busybox
crictl imges
crictl ps -a
crictl pods << pod 리스트 출력됨
```

### 요약
- ctr : 안씀
- nerdcrl : 쓸꺼임 dockerCLI 랑 비슷함
- crictl : 디버깅용으로만 쓰는데, 쿱커뮤니티에서 만든거임


<br><br><br><br>

## 13. ETCD For Beginners

- etcd 관련 배울점
  - 이게 뭔지
  - key-value 스토어
  - 가볍게 시작하기
  - 써먹는방법
  - 분산시스템
  - RAFT 프로토컬
  - 베스트 프렉티스

### ETCD
- key-value 스토어 형식임 : HashMap 이나 레디스같은거.. 
- JSON 이나 YAML 형식 씀
- etcd 컨트롤 클라이언트 설치해서 쓸수도 있음

### ETCD 버전정보
- v2 랑 v3 랑 차이가 많다
- 이 명령어로 버전 확인 `etcdctl --version`
- ectd 버전3 라도 api 버전2 로 동작도 가능하다

<br><br><br>

## 14. ETCD in Kubernetes

### etcd 가 쿱에서 어떻게? 어떤 부분에서 사용되나??
```
nodes
pods
configs
secrets
accounts
roles
bindings
other....
```
- 그냥 모든곳에 다쓰이는 근간이라고 할수 있다
- 쿠버네티스에 뭔가를 하는거가 그냥 etcd 서버에 뭔가를 변경하는거랑 같음
- etcd 설치하고 싶음 : 깃헙 coreos/etcd 가서 다운받기

```
$ kubectl get pods -n kube-system

NAME                                              READY   STATUS      RESTARTS   AGE
calico-kube-controllers-@@@@@@@@@@-kzm6v          1/1     Running     1          185d
calico-node-@@@@@                                 1/1     Running     1          185d
coredns-@@@@@@@@@@@@@@                            1/1     Running     3          424d
coredns-secondary--@@@@@@@@@@@@@@@                1/1     Running     2          424d
csi-cinder-controllerplugin-@                     6/6     Running     0          5d7h
csi-cinder-nodeplugin-@@@@@                       3/3     Running     0          5d7h
dns-autoscaler--@@@@@@@@@@@@@@                    1/1     Running     2          424d
dns-autoscaler-secondary-@@@@@@@@@@@@@@@          1/1     Running     2          424d
kube-apiserver-@@@@@@@@@@@@@-master-1             1/1     Running     2          517d
kube-apiserver-@@@@@@@@@@@@@-master-2             1/1     Running     5          517d
kube-apiserver-@@@@@@@@@@@@@-master-3             1/1     Running     3          243d
kube-controller-manager-@@@@@@@@@@@@@-master-1    1/1     Running     22         183d
kube-controller-manager-@@@@@@@@@@@@@-master-2    1/1     Running     15         183d
kube-controller-manager-@@@@@@@@@@@@@-master-3    1/1     Running     16         183d
kube-proxy-@@@@@                                  1/1     Running     0          108d
kube-proxy-@@@@@                                  1/1     Running     0          144d
kube-scheduler-@@@@@@@@@@@@@-master-1             1/1     Running     15         183d
kube-scheduler-@@@@@@@@@@@@@-master-2             1/1     Running     17         183d
kube-scheduler-@@@@@@@@@@@@@-master-3             1/1     Running     21         183d
@@@@@@@-fluent-bit-@@@@@                          1/1     Running     0          179d
local-volume-provisioner-@@@@@                    1/1     Running     4          490d
metrics-server-@@@@@@@@@@-@@@@@                   2/2     Running     1          144d
```

- 만들다가 망한 장난감 1호기 뭐가 적혀있는지 찾아봤음

- HA 구성은 마스터노드 여러개여야함
  - 엣시디 서버도 마스터마다 각각 존재한다는건가?? 궁금함(@@@)


<br><br><br><br>


## 16. Kube-API Server
-  Kube-API Server 가 하는dlf

```
[kube-apiserver]
- 1) kubectl 명령어 입력받아서
- 2) 유효성 검증하고
- 3) ETCD 클러스터에다가 검색
- 4) ETCD 클러스터에다가 변경사항 업데이트

[master]

[worker code]

[kube-scheduler]
- 5) 스케쥴러가 돌면 kube-apiserver 가 일하게 만듬

[???]
- 6) Kublet : 뭔지모름

==============================
사용자 : kubectl 명령어 입력시
```


### 팁
- 큐브adm 같은걸로 돌리면 이런거 몰라도 되는데
- 하드웨이로 한땀한땀 구성했으면.. 알고있을꺼다
- 마스터 하나마다 api-server 하나씩 있음



## 17 컨트롤러
- 내가 선언한(원하는) 상태를 유지해줌
  - 지속적으로 모니터링
  - KubeAPI 서버를 통해서
-  컨트롤러 스페이스
  - 포드 컨트롤러
  - 레플리케이션 컨트롤러
  - 어쩌구저쩌고 다양한 컨트롤러 많음
- 큐브 컨트롤러 매니져
  - 모든 컨트롤러가 들어있는 서비스임
  - 큐브-컨트롤러-매니져 >> 다운로드받아서 뭐 하는 방법이 있긴 함..


## 18. Kube Scheduler

- 스케쥴러는 "위치"만을 결정
- 어떤 포드가 어떤 노드에 배치되어야 하는지만 검증
- 노드(VM의 쿱 추상) 보고 가장 적당한 노드에다가 배치
- 관련주제
  - 리소스 requirements, Limits
  - Taints, Tolerantions
  - Node Selector / Affinity
  


## 19. Kubelet

- 배의 선장같음. 
```
선내의 모든 활동을 지휘
무리의 일원이 되기 위해 필요한ㅡ모든 서류를 처리하죠
마스터와의 유일한 연락망
선박에 컨테이너를 싣거나 내립니다
주 운항 일정 관리자의 지시에 따라서요
선박과 컨테이너의 상태를일정 간격으로 보고하죠
쿠베르네테스 워커 노드로 클러스터로 노드를 등록해요
```
- 이미지엔진(도커)한테 실행하라고 명령도 큐블렛이 내림
- 큐벨렛은 포드의 상태와 컨테이너를 계속 모니터링하고 동시에 kube API를 통해서 보고함



## 20. Kube Proxy

- 도커 프로세스 돌리면 격리되는게 너무 당연하죠?
- 근데 쿠버네티스 안에서는 pod(도커 프로세스 같은)끼리는 통신이 원활이 잘 되잖아요? 이게 큐브프록시가 있어서임
 - 모든 포드가 연결되도록 클러스터를 연결시켜주는 네트워크임
- POD 에 IP 도 할당해줄수 있고, IP 가지고 찾아갈수도 있는데(마치 DNS 같은기능도 있음) IP는 바뀔수 있음 
- IP 바뀌는거 대비해서 쿠버네티스 내부 도메인 스면됨 


## 21. Recap - Pods

- 포드 : 앱 인스턴스 한개 >> 프로스스 하나 , 혹은 jar파일 실행한거 하나라고 보면 됨. (그런데 거기에 도커프로세스를 얹은) 
- 포드랑 앱 인스턴스랑은 1:1 관계이어야만 함
- 포드 개념이 있으니까 >> APP 레이어의 스케일아웃이 아주 편함
- 헬퍼앱이랑 , 앱 인스턴스랑 한포드에 두 인스턴스가 같이 지내는경우가 있는데, 이러면 통신도 간단하고 디스크 공유도 편함

## 22. Pods with YAML

- 쿱은 입력으로 YAML 파일을 받음 >> 쿱을 컨트롤하는 얌을 배울꺼임

- 가장먼저 쿱 얌파일에 기본적으로 있어야하는거 4가지
```yaml
apiVersion: #
kind:
metadata:
# ...
spec:
# ...
```

## 23. Demo - Pods with YAML
## 24. Practice Test Introduction